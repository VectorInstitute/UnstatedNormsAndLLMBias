{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from IPython.display import clear_output, display\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import confusion_matrix as ConfusionMatrix\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Constants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will visualize how each of the following variable affects the fairness towards a particular protected group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEPENDENT_VARIABLES = [\"model\", \"dataset\", \"num_params\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metric_name(metric_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Prettify diagram texts by replacing\n",
    "    snake case with regular title case.\n",
    "    \"\"\"\n",
    "    output_words = []\n",
    "    for word in metric_name.split(\"_\"):\n",
    "        word = word.title() if word not in [\"FPR\", \"TPR\"] else word.upper()\n",
    "        output_words.append(word)\n",
    "\n",
    "    return \" \".join(output_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Predictions Ready"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Prediction File\n",
    "Load the prediction `tsv` file from the provided paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_filenames(path_to_dir: str, template_name: str, suffix: str = \".tsv\") -> List[str]:\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(path_to_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(suffix) and template_name in file:\n",
    "                all_files.append(os.path.join(root, file))\n",
    "    return all_files\n",
    "\n",
    "\n",
    "repo_abs_path = Path(os.path.abspath(\"\")).parent.parent.parent\n",
    "predictions_dir = f\"{repo_abs_path}/unstated_norms_llm_bias/prompt_based_classification/predictions\"\n",
    "prediction_tsv_paths = find_csv_filenames(predictions_dir, template_name=\"\")\n",
    "\n",
    "plots_dir = f\"{repo_abs_path}/unstated_norms_llm_bias/visualization/plots\"\n",
    "plot_name = \"pure_prompt\"\n",
    "\n",
    "output_folder: str = \"stats\"\n",
    "output_table = pd.DataFrame()\n",
    "\n",
    "for prediction_tsv_path in tqdm(prediction_tsv_paths, ncols=80):\n",
    "    prediction_tsv_filename = os.path.basename(prediction_tsv_path)\n",
    "    dataframe = pd.read_csv(prediction_tsv_path, delimiter=\"\\t\")\n",
    "    del dataframe[\"text\"]\n",
    "    if \"cot\" in prediction_tsv_path:\n",
    "        dataframe[\"dataset\"] = \"CoT\"\n",
    "    dataframe = dataframe[dataframe[\"category\"] == \"grouped_race\"]\n",
    "    if output_table is None:\n",
    "        output_table = dataframe\n",
    "    else:\n",
    "        assert isinstance(output_table, pd.DataFrame)\n",
    "        output_table = pd.concat([output_table, dataframe])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Statistics\n",
    "Since there can be multiple examples for each protected group, we apply the `groupby` method to compute aggregated statistics across all the examples of each protected group.\n",
    "\n",
    "Refer to the following function for details on how we implemented the metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for aggregating statistics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find a function that takes in a dataframe. This dataframe is will be automatically generated using the `pd.DataFrame.groupby`. The content of this dataframe is a subset of one particular prediction tsv for a particular protected group (e.g., the \"young\" group for the protected class \"age\"). \n",
    "\n",
    "The function will return a `pd.Series` (like a dictionary) mapping the following fairness metrics to their floating point values:\n",
    "- Accuracy\n",
    "- TPR(Positive)\n",
    "- TPR(Neutral)\n",
    "- TPR(Negative)\n",
    "- FPR(Positive)\n",
    "- FPR(Neutral)\n",
    "- FPR(Negative)\n",
    "\n",
    "Also refer to https://stackoverflow.com/a/50671617 for details on how the TPR and FPR values are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_filter = 5\n",
    "unique_runs_with_accuracy = output_table[[\"model\", \"run_id\", \"dataset\"]].drop_duplicates()\n",
    "unique_runs_with_accuracy = unique_runs_with_accuracy.sort_values([\"model\", \"dataset\"], ascending=False)\n",
    "best_runs_by_model_dataset = unique_runs_with_accuracy.groupby([\"model\", \"dataset\"]).head(top_n_filter).reset_index()\n",
    "run_ids_to_keep = best_runs_by_model_dataset[\"run_id\"].tolist()\n",
    "output_table = output_table.loc[output_table[\"run_id\"].isin(run_ids_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_model_name(model_name: str) -> str:\n",
    "    model_name = model_name.replace(\"facebook/\", \"\")\n",
    "    model_name = model_name.replace(\"data/models/\", \"\")\n",
    "    model_name = model_name.replace(\"model-weights/\", \"\")\n",
    "    model_name = model_name.replace(\" CoT\", \"\")\n",
    "    if model_name.startswith(\"opt\"):\n",
    "        return model_name.replace(\"opt\", \"OPT\").upper()\n",
    "    elif model_name.startswith(\"roberta\"):\n",
    "        return model_name.replace(\"roberta\", \"RoBERTa\")\n",
    "    elif \"Llama2\" in model_name:\n",
    "        return model_name.replace(\"Llama2\", \"Llama-2\")\n",
    "    else:\n",
    "        return model_name\n",
    "\n",
    "\n",
    "def modify_dataset_name(dataset_name: str) -> str:\n",
    "    if \"SST5\" == dataset_name:\n",
    "        return \"SST5 Grouped 9-Shot Prompt\"\n",
    "    if \"CoT\" == dataset_name:\n",
    "        return \"Zero-Shot CoT\"\n",
    "    elif \"SemEval\" in dataset_name:\n",
    "        return \"SemEval 9-Shot Prompt\"\n",
    "    elif \"ZeroShot\" in dataset_name:\n",
    "        return \"Zero-Shot Prompt\"\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(dataframe: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Input: dataframe representing all predictions for a particular\n",
    "    protected group in a particular run.\n",
    "    \"\"\"\n",
    "    output: Dict[str, float] = {}\n",
    "\n",
    "    num_examples = len(dataframe)\n",
    "    num_correct = np.sum(dataframe[\"y_pred\"] == dataframe[\"y_true\"])\n",
    "\n",
    "    output[\"accuracy\"] = num_correct / num_examples\n",
    "\n",
    "    # See https://stackoverflow.com/a/50671617\n",
    "    confusion_matrix = ConfusionMatrix(dataframe[\"y_true\"], dataframe[\"y_pred\"], labels=[0, 1, 2])\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    eps = 10**-10\n",
    "\n",
    "    FP = np.copy(FP) + eps\n",
    "    FN = np.copy(FN) + eps\n",
    "    TP = np.copy(TP) + eps\n",
    "    TN = np.copy(TN) + eps\n",
    "\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (TP + FN)\n",
    "    TPR = 1 - FNR\n",
    "\n",
    "    for label_index, label in enumerate([\"negative\", \"neutral\", \"positive\"]):\n",
    "        output[label + \"_FPR\"] = FPR[label_index]\n",
    "        output[label + \"_TPR\"] = TPR[label_index]\n",
    "\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Per-Group Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_dataframe = output_table.groupby([*INDEPENDENT_VARIABLES, \"run_id\", \"category\", \"group\"]).apply(get_stats)\n",
    "analyzed_dataframe.query('category == \"grouped_race\"')[[\"positive_TPR\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table = analyzed_dataframe.reset_index()\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_table.to_csv(os.path.join(output_folder, \"aggregated.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, note that some of the protected classes include a large number of groups. You may filter on the protected groups to include in each category. If a category isn't in this dictionary, the visualizations will include all the groups under that category by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_GROUPS = {\n",
    "    \"grouped_religion\": [\"christianity\", \"islam\", \"judaism\"],\n",
    "    \"grouped_disability\": [\n",
    "        \"without\",\n",
    "        \"cognitive\",\n",
    "        \"physical\",\n",
    "        \"hearing\",\n",
    "        \"sight\",\n",
    "        \"chronic_illness\",\n",
    "        \"mobility\",\n",
    "        \"mental_health\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "groups_to_include = []\n",
    "for category in output_table[\"category\"].unique():\n",
    "    groups = SELECTED_GROUPS.get(category)\n",
    "    if groups is None:\n",
    "        groups = list(set(output_table.query(f'category == \"{category}\"')[\"group\"]))\n",
    "\n",
    "    groups_to_include.extend(groups)\n",
    "\n",
    "print(\"Groups selected:\", \", \".join(groups_to_include))\n",
    "output_table = output_table.query(\"group == @groups_to_include\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Gap\": How each group deviates from the category mean\n",
    "Below, we will calculate how far each group deviates from the mean of that category. For example, how far does the accuracy on examples mentioning the \"young\" group deviate from the mean of all examples of the \"age\" category?\n",
    "\n",
    "If there are more than one runs for each model, we would calculate gap metrics for each run separately. \n",
    "\n",
    "We use a for loop to add the following metrics of interest to our list:\n",
    "- Accuracy\n",
    "- False Positive Rates\n",
    "    - FPR(\"Negative\")\n",
    "    - FPR(\"Neutral\")\n",
    "    - FPR(\"Positive\")\n",
    "- True Positive Rates\n",
    "    - TPR(\"Negative\")\n",
    "    - TPR(\"Neutral\")\n",
    "    - TPR(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"accuracy\"]\n",
    "for label in [\"negative\", \"neutral\", \"positive\"]:\n",
    "    metrics.append(label + \"_FPR\")\n",
    "    metrics.append(label + \"_TPR\")\n",
    "\n",
    "gap_metrics = []\n",
    "for metric in metrics:\n",
    "    gap_metrics.append(f\"{metric}_gap\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval Calculations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step, we've found the gaps for each (metric, category, group, model, run/seed). If we have more than one runs/seeds for each model, we can aggregate the results to find a confidence interval for each gap variable.\n",
    "\n",
    "To do so, we will aggregate the previous table (metric, category, group, model, run/seed) along the run/seed axis. The result will be a table with indices (metric, category, group, model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_table = (\n",
    "    output_table[[*INDEPENDENT_VARIABLES, \"category\", \"group\", \"accuracy\"]]\n",
    "    .groupby([*INDEPENDENT_VARIABLES, \"category\", \"group\"])\n",
    "    .agg([\"mean\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric, gap_metric in zip(metrics, gap_metrics):\n",
    "    group_by = [*INDEPENDENT_VARIABLES, \"category\", \"run_id\"]\n",
    "    grouped = output_table[[*INDEPENDENT_VARIABLES, \"category\", \"run_id\", metric]].groupby(group_by)\n",
    "    group_mean = grouped.transform(\"mean\")\n",
    "    normalized_values = output_table[metric] - group_mean[metric]\n",
    "    output_table[gap_metric] = normalized_values\n",
    "\n",
    "    stats = (\n",
    "        output_table[[*INDEPENDENT_VARIABLES, \"category\", \"group\", gap_metric]]\n",
    "        .groupby([*INDEPENDENT_VARIABLES, \"category\", \"group\"])\n",
    "        .agg([\"mean\", \"count\", \"std\"])\n",
    "    )\n",
    "\n",
    "    mean_values = []\n",
    "    ci_width = []\n",
    "    lower_values = []\n",
    "    upper_values = []\n",
    "    hypothesis_selected = []\n",
    "    z_score = 1.96\n",
    "\n",
    "    # Calculate 95% Confidence interval for each\n",
    "    # (metric, category, group, model).\n",
    "    for index in stats.index:\n",
    "        mean, n, stdev = stats.loc[index]  # type: ignore\n",
    "        lower = mean - z_score * stdev / sqrt(n)\n",
    "        upper = mean + z_score * stdev / sqrt(n)\n",
    "\n",
    "        mean_values.append(mean)\n",
    "        ci_width.append(z_score * stdev / sqrt(n))\n",
    "        lower_values.append(lower)\n",
    "        upper_values.append(upper)\n",
    "\n",
    "        if lower > 0:\n",
    "            hypothesis_selected.append(1)\n",
    "        elif upper < 0:\n",
    "            hypothesis_selected.append(-1)\n",
    "        else:\n",
    "            hypothesis_selected.append(0)\n",
    "\n",
    "    stats_table[gap_metric] = mean_values\n",
    "    stats_table[gap_metric + \"_width\"] = ci_width\n",
    "    stats_table[gap_metric + \"_lower\"] = lower_values\n",
    "    stats_table[gap_metric + \"_upper\"] = upper_values\n",
    "\n",
    "stats_table = stats_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_table[\"model\"] = stats_table[\"model\"].apply(modify_model_name)\n",
    "stats_table[\"dataset\"] = stats_table[\"dataset\"].apply(modify_dataset_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare between Groups of each category \n",
    "E.g. all groups in the \"age\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"(all)\", *(stats_table[\"dataset\"].unique())]\n",
    "models = [\"(all)\", *(stats_table[\"model\"].unique())]\n",
    "categories = stats_table[\"category\"].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interactive Visualization Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selector = widgets.Dropdown(options=models, value=models[0], description=\"Model:\")\n",
    "category_selector = widgets.Dropdown(options=categories, value=categories[0], description=\"Category:\")\n",
    "dataset_selector = widgets.Dropdown(options=datasets, value=datasets[0], description=\"Dataset:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fn(button: Any) -> None:\n",
    "    clear_output()\n",
    "    display(model_selector, category_selector, dataset_selector, button)\n",
    "\n",
    "    model = model_selector.value\n",
    "    category = category_selector.value  # type: ignore\n",
    "    dataset = dataset_selector.value  # type: ignore\n",
    "    filter_cond_1 = stats_table[\"category\"] == category\n",
    "    filter_cond_2 = stats_table[\"model\"] == model if model != \"(all)\" else True\n",
    "    filter_cond_3 = stats_table[\"dataset\"] == dataset if dataset != \"(all)\" else True\n",
    "    filtered_table = stats_table[filter_cond_1 & filter_cond_2 & filter_cond_3]\n",
    "    filtered_table[\"model - dataset\"] = filtered_table[\"model\"] + \" \" + filtered_table[\"dataset\"]\n",
    "    if filtered_table.empty:\n",
    "        print(\"None of the runs in the table matched the selections.\")\n",
    "        return\n",
    "\n",
    "    category_name = category.split(\"_\")[-1]\n",
    "    title = f\"Gap for {category_name.title()}<br>{dataset}\"\n",
    "\n",
    "    gen_fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        row_heights=[1000, 1000],\n",
    "        row_titles=(\"Negative Sentiment FPR Gap\", \"Positive Sentiment FPR Gap\"),\n",
    "        vertical_spacing=0.03,\n",
    "    )\n",
    "\n",
    "    for annotation in gen_fig[\"layout\"][\"annotations\"]:\n",
    "        annotation[\"font\"] = dict(size=24, family=\"Helvetica\")  # Setting size to 20 and font family to Courier\n",
    "\n",
    "    for i, metric in enumerate([\"negative_FPR_gap\", \"positive_FPR_gap\"]):\n",
    "        fig = px.scatter(\n",
    "            filtered_table,\n",
    "            x=\"group\",\n",
    "            y=metric,\n",
    "            color=\"model - dataset\",\n",
    "            error_y=metric + \"_width\",\n",
    "            labels={\"group\": \"Protected Group\", metric: format_metric_name(metric)},\n",
    "            category_orders={\n",
    "                \"model - dataset\": [\n",
    "                    \"OPT-6.7B Zero-Shot Prompt\",\n",
    "                    \"OPT-6.7B SST5 Grouped 9-Shot Prompt\",\n",
    "                    \"OPT-6.7B SemEval 9-Shot Prompt\",\n",
    "                    \"LLaMA-7B Zero-Shot Prompt\",\n",
    "                    \"LLaMA-7B Zero-Shot CoT\",\n",
    "                    \"LLaMA-7B SST5 Grouped 9-Shot Prompt\",\n",
    "                    \"LLaMA-7B SemEval 9-Shot Prompt\",\n",
    "                    \"Llama-2-7B Zero-Shot Prompt\",\n",
    "                    \"Llama-2-7B Zero-Shot CoT\",\n",
    "                    \"Llama-2-7B SST5 Grouped 9-Shot Prompt\",\n",
    "                    \"Llama-2-7B SemEval 9-Shot Prompt\",\n",
    "                ]\n",
    "            },\n",
    "            title=title,\n",
    "        )\n",
    "\n",
    "        customize_traces(fig, i)\n",
    "\n",
    "        fig.for_each_xaxis(lambda x: x.update(title=\"\"))\n",
    "        fig.for_each_annotation(lambda a: a.update(text=\"\"))\n",
    "        for trace in fig.data:\n",
    "            gen_fig.add_trace(trace, row=i + 1, col=1)\n",
    "\n",
    "    customize_figure_layout(gen_fig)\n",
    "    save_and_display_figure(gen_fig, plot_name, plots_dir)\n",
    "\n",
    "\n",
    "def customize_traces(fig: Any, row_index: int) -> None:\n",
    "    \"\"\"Customize traces with larger markers and adjusted legend visibility.\"\"\"\n",
    "    legend_show = row_index == 0  # Show legend only in the first row\n",
    "    marker_size = 10  # Increased marker size for better visibility\n",
    "    error_y_thickness = 3.5  # Thicker error bars\n",
    "\n",
    "    color_map = {\n",
    "        \"OPT-6.7B Zero-Shot Prompt\": \"#bfdb81\",\n",
    "        \"OPT-6.7B SST5 Grouped 9-Shot Prompt\": \"#83a561\",\n",
    "        \"OPT-6.7B SemEval 9-Shot Prompt\": \"#48723e\",\n",
    "        \"LLaMA-7B Zero-Shot Prompt\": \"#f3ccff\",\n",
    "        \"LLaMA-7B Zero-Shot CoT\": \"#d896ff\",\n",
    "        \"LLaMA-7B SST5 Grouped 9-Shot Prompt\": \"#be29ec\",\n",
    "        \"LLaMA-7B SemEval 9-Shot Prompt\": \"#800080\",\n",
    "        # https://www.color-hex.com/color-palette/97036\n",
    "        \"Llama-2-7B Zero-Shot Prompt\": \"#afe3ff\",\n",
    "        \"Llama-2-7B Zero-Shot CoT\": \"#53abff\",\n",
    "        \"Llama-2-7B SST5 Grouped 9-Shot Prompt\": \"#0060ff\",\n",
    "        \"Llama-2-7B SemEval 9-Shot Prompt\": \"#0034c3\",\n",
    "    }\n",
    "\n",
    "    for trace in fig.data:\n",
    "        model = trace.legendgroup\n",
    "        trace.showlegend = legend_show\n",
    "        trace.marker.size = marker_size\n",
    "        trace.error_y.thickness = error_y_thickness\n",
    "        if model in color_map:\n",
    "            trace.marker.color = color_map[model]\n",
    "        if \"OPT\" in model:\n",
    "            trace.marker.symbol = \"diamond\"\n",
    "        elif \"LLaMA\" in model:\n",
    "            trace.marker.symbol = \"square\"\n",
    "\n",
    "\n",
    "def customize_figure_layout(gen_fig: Any) -> None:\n",
    "    font_size = 24\n",
    "\n",
    "    gen_fig.update_layout(\n",
    "        height=1200,\n",
    "        width=1300,\n",
    "        legend=dict(\n",
    "            y=1.25,\n",
    "            x=0.48,\n",
    "            xanchor=\"center\",\n",
    "            orientation=\"h\",\n",
    "            valign=\"top\",\n",
    "            title_text=\"\",\n",
    "            font=dict(size=font_size),\n",
    "            title_font_family=\"Helvetica\",\n",
    "        ),\n",
    "        scattermode=\"group\",\n",
    "        font_color=\"black\",\n",
    "        title_font_family=\"Helvetica\",\n",
    "        title_x=0.47,\n",
    "        margin=dict(l=10, r=10, t=10, b=10),\n",
    "        plot_bgcolor=\"#eeeeee\",\n",
    "        font=dict(size=font_size, family=\"Helvetica\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def save_and_display_figure(gen_fig: Any, template_name: str, plots_dir: str) -> None:\n",
    "    plot_png_path = f\"{plots_dir}/{plot_name}_FPR_Gaps.png\"\n",
    "    plot_pdf_path = f\"{plots_dir}/{plot_name}_FPR_Gaps.pdf\"\n",
    "\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    gen_fig.write_image(plot_png_path, scale=2)\n",
    "    gen_fig.write_image(plot_pdf_path)\n",
    "\n",
    "    with open(plot_png_path, \"rb\") as img_file:\n",
    "        image_widget = widgets.Image(value=img_file.read(), format=\"png\", width=600, height=650)\n",
    "        display(image_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = widgets.Button(description=\"Visualize\")\n",
    "button.on_click(visualize_fn)\n",
    "visualize_fn(button)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25f6c606dd86a3cae59df91b75f7a72b0b2bf1388cda4c8c160c9bd2d7ceb352"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
